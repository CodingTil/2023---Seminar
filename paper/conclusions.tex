\section{Conclusions}
This paper provided a comprehensive exploration of the different methodologies employed to understand and visualize data movements and accesses in computer programs, an imperative for optimizing performance. The data acquisition processes are based on three principal methodologies, which can often be combined to achieve a more comprehensive analysis. Dynamic Analysis, involving the execution of a program to gather data, provides accurate results, albeit time-consuming. Static Analysis, on the other hand, is a swift method that just analyzes the source code, bypassing the need for program execution, but can sometimes produce insufficient insights. Lastly, Cache Simulation offers an in-depth understanding of a program's interaction with the memory hierarchy, allowing for a meticulous examination of its performance.

Once the data locality information is collected, it becomes essential to present it in an intuitive yet informative way. Here, visualizations play a critical role. These visualizations, varying in their granularity, offer insights into different levels of the program, from high-level overviews to fine-grained dissections of memory usage. The use of multiple visualizations in tandem empowers performance engineers to gain a comprehensive understanding of data movements and accesses, leading to effective bottleneck resolution.

In this regard, we delved into three prominent works that provide unique tools for visualizing data movements and accesses. Each work was presented and compared, highlighting their distinctive contributions to the wider landscape of methods employed in this domain.

The broad-ranging implications of these methods were underscored, highlighting their relevance to not just high-performance computing (HPC) but to any application that would benefit from performance optimization. This paper thus provides a solid foundation for understanding and visualizing data movements and accesses, a crucial aspect of programming and performance engineering.

As we look to the future, the field holds exciting prospects. The continuous refinement of data gathering methods and visualizations is one aspect, but the ultimate goal extends to automatic program optimization. Initiatives have already been launched to develop algorithms for automatic optimization \cite{calotoiu2022lifting}, significantly easing the programmer's workload. Such advancements could be especially beneficial for domain researchers, allowing them to focus more on their domain-specific work.

The emergence of machine learning, particularly deep learning, also presents new avenues for program optimization. Preliminary work in this area indicates that future compilers might utilize machine learning for automatic, compile-time program optimization \cite{cummins2021programl}.

In conclusion, the tools and methods presented in this paper provide a solid foundation for understanding and visualizing data movements and accesses, a crucial aspect of optimizing program performance. With ongoing advancements in automatic program optimization and the advent of machine learning techniques, the future looks bright for further improvements in this vital aspect of programming and performance engineering.