\section{Data Gathering and Visualization Approaches}\label{sec:methods}
In the pursuit of optimizing a program's data locality, implementing visual aids to represent data movements and data layouts can be particularly helpful. This approach enables quick and effective identification of data-related issues, their comprehension, and ultimately, their resolution. This method empowers not only program optimization experts but also domain researchers to effortlessly optimize their programs.

To enable such effective visualization, however, it's essential to first collect information regarding data locality. Several studies have explored this area, leading to the identification of three primary strategies: Dynamic Analysis, Static Analysis, and Simulation. These strategies, which we'll delve into in Sections \ref{sec:dynamic_analysis}, \ref{sec:static_analysis}, and \ref{sec:simulation}, each bring their unique benefits and drawbacks. Furthermore, it's important to note that some techniques used for gathering data locality information may not be confined to just one of these three fundamental categories, and could instead exhibit characteristics of multiple approaches.

Once this data locality information is gathered, it needs to be presented in a user-friendly manner. There exists a wide variety of visualization techniques that can fulfill this requirement, some of which we will detail in Section \ref{sec:visualization}.

\subsection{Dynamic Analysis}\label{sec:dynamic_analysis}
Dynamic analysis of a program's data locality involves executing the program while concurrently gathering memory-related data and statistics. A straightforward approach to dynamic analysis is to run the program, extract hardware counters such as cache misses, and analyze them. This approach, however, does not provide a holistic view of the program's behavior because it lacks contextual information.

To gain a deeper understanding of program performance beyond general statistics, dynamic analysis uses more nuanced techniques like profiling, statistical sampling, and tracing \cite{shende1999profiling,itzkowitz2003memory,gimenez2017memaxes,mckinley1999quantifying,adhianto2010hpctoolkit}.

Profiling involves periodically interrupting the program's execution to capture both hardware-derived attributes and context-related information \cite{itzkowitz2003memory,gimenez2017memaxes,adhianto2010hpctoolkit}. Profiling techniques analyze the program's call stack and program counter to provide specific details such as the current line of code being executed, the symbol, and, for arrays, the accessed index. This information facilitates the derivation of deeper metrics, such as the number of cache misses per array \cite{adhianto2010hpctoolkit}. Profiling typically focuses on memory-related events, but the constant interruption can increase runtime overhead. Hence, a trade-off between the granularity and the quality of the measurements is necessary.

Statistical sampling is an alternative approach related to profiling. It captures the program's state at fixed time intervals rather than event-triggered interruptions. The advantage of statistical sampling is that it avoids frequent interruption of the program's execution, reducing the runtime overhead. However, high-quality measurements require sufficiently high sampling rates to capture all relevant details \cite{adhianto2010hpctoolkit}.

Tracing, another technique, allows for a temporal understanding of a program's behavior by logging event-specific data over time. Tracing functions by documenting specific events or functions during program execution, providing a chronological account of these events and their corresponding data \cite{shende1999profiling,adhianto2010hpctoolkit,mckinley1999quantifying}.

In conclusion, dynamic analysis offers several distinct advantages in the study of a program's behavior concerning data locality. As the program is being executed, it offers more precise practical insights into hardware oriented data locality optimization. Further, dynamic analysis can be employed in conjunction with actual data, making it more representative of real-world scenarios.

However, it's important to note the inherent disadvantages of dynamic analysis. The act of running an entire program can be time-consuming and costly, particularly for larger and more complex software. Additionally, isolating and scrutinizing specific parts of a program under the dynamic analysis approach can be complicated, especially when compared to static analysis methods (Section \ref{sec:static_analysis}).

\subsection{Static Analysis}\label{sec:static_analysis}
\textit{Analyze the program statically using a compiler}
\cite{schaad2022boosting,schaad2021boosting,ben2019statefulSDFG,matwin1985prograph,kodosky2020labview,calotoiu2022lifting,ben2023bridging}

Advantages:
\begin{itemize}
  \item Can be used to analyze specific parts of the program
  \item Fast and cheap → No need to run program
\end{itemize}
Disadvantages:
\begin{itemize}
  \item Needs to be parameterized for specific hardware (and often not accurate enough → might miss some details)
  \item Can not be used in combination with actual data
\end{itemize}

\subsection{Simulation}\label{sec:simulation}
\textit{Simulate the program on a simulator}
\cite{schaad2022boosting,hammer2017kerncraft,choudhury2011abstract,iakymchuk2012modeling}

Advantages:
\textbf{TODO}
\begin{itemize}
  \item Can be used to analyze specific parts of the program
  \item In between Static and Dynamic Analysis in terms of precision and speed and cost
\end{itemize}
Disadvantages:
\textbf{TODO}

\subsection{Visualization Techniques}\label{sec:visualization}
\textit{Very brief overview of different visualizations (Colored Graphs, Heatmaps, etc.)}

\textit{From HPC Toolkit: Typical metrics such as elapsed time
are useful for identifying program hot spots. However, tuning a program usually requires a measure
of not where resources are consumed, but where they are consumed inefficiently. For this purpose,
derived measures such as the difference between peak and actual performance are far more useful than
raw data such as operation counts. HPCTOOLKIT’s hpcviewer user interface supports computation
of user-defined derived metrics and enables users to rank and sort program scopes using such metrics.}
